{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER/POS tagging and chunking \n",
    "\n",
    "O means the word doesnâ€™t correspond to any entity.\n",
    "B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
    "B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
    "B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
    "B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the conll2003 dataset for fine tuning purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conll2003 = datasets.load_dataset(\"conll2003\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003['train'].features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003['train'].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the tokenizer for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text=conll2003['train'][0]\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input=tokenizer(example_text['tokens'], is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'eu',\n",
       " 'rejects',\n",
       " 'german',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'british',\n",
       " 'lamb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids=tokenized_input.word_ids()\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['ner_tags']), len(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the length of the tags/number of words is different than the length of the tokenized inputs. This is because transformers are pretrained on stems/lemma at a sub-word level, and can also add special tokens such as [CLS] and [SEP] tags to the tokenizer input. Writing a function below to align the tags and words. \n",
    "\n",
    "1. set -100 has the label for the special tokens\n",
    "2. mask subword representations after the first subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True): \n",
    "    \"\"\"\n",
    "    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n",
    "    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n",
    "\n",
    "    Parameters:\n",
    "    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n",
    "                     - \"tokens\": list of words in a sentence.\n",
    "                     - \"ner_tags\": list of corresponding entity tags for each word.\n",
    "                     \n",
    "    label_all_tokens (bool): A flag to indicate whether all tokens should have labels. \n",
    "                             If False, only the first token of a word will have a label, \n",
    "                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n",
    "\n",
    "    Returns:\n",
    "    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n",
    "    labels = [] \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token. \n",
    "        previous_word_idx = None \n",
    "        label_ids = []\n",
    "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None \n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids: \n",
    "            if word_idx is None: \n",
    "                # set â€“100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token                 \n",
    "                label_ids.append(label[word_idx]) \n",
    "            else: \n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "                # mask the subword representations after the first subword\n",
    "                 \n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids) \n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    return tokenized_inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=tokenize_and_align_labels(conll2003['train'][4:5])\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset=conll2003.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Harsh Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\"test-ner\",evaluation_strategy = \"epoch\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=3, weight_decay=0.01, ) \n",
    "data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    "metric=datasets.load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=conll2003['train'][0]\n",
    "label_list=conll2003['train'].features['ner_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[label_list[i] for i in example['ner_tags']]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels=eval_preds\n",
    "    pred_logits=np.argmax(pred_logits, axis=2)\n",
    "      # We remove all the values where the label is -100\n",
    "    \n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "   ] \n",
    "    results = metric.compute(predictions=predictions, references=true_labels) \n",
    "    return { \n",
    "   \"precision\": results[\"overall_precision\"], \n",
    "   \"recall\": results[\"overall_recall\"], \n",
    "   \"f1\": results[\"overall_f1\"], \n",
    "  \"accuracy\": results[\"overall_accuracy\"], \n",
    "  } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer( model, args, train_dataset=tokenized_dataset[\"train\"], eval_dataset=tokenized_dataset[\"validation\"], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba6533311fe48a4bb06d948d0a760ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2264, 'grad_norm': 1.5905286073684692, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3496cfaa802d4423b2d297a600032bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06011531502008438, 'eval_precision': 0.9217227635075288, 'eval_recall': 0.931312227318492, 'eval_f1': 0.9264926826553891, 'eval_accuracy': 0.9831604365577391, 'eval_runtime': 208.5639, 'eval_samples_per_second': 15.583, 'eval_steps_per_second': 0.978, 'epoch': 1.0}\n",
      "{'loss': 0.0702, 'grad_norm': 0.7943836450576782, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0463, 'grad_norm': 2.2683959007263184, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e0b5aa67ba4737b83e201275e864df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05536239966750145, 'eval_precision': 0.9345804737657737, 'eval_recall': 0.9445128090390424, 'eval_f1': 0.9395203916986591, 'eval_accuracy': 0.9858293484995313, 'eval_runtime': 239.8188, 'eval_samples_per_second': 13.552, 'eval_steps_per_second': 0.851, 'epoch': 2.0}\n",
      "{'loss': 0.0347, 'grad_norm': 1.6674907207489014, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n",
      "{'loss': 0.0259, 'grad_norm': 2.8925271034240723, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83da0e1cec9d4cd58aafd130b625364e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05624592304229736, 'eval_precision': 0.9388636615350537, 'eval_recall': 0.9483163664839468, 'eval_f1': 0.943566340160285, 'eval_accuracy': 0.9865283492461913, 'eval_runtime': 206.0349, 'eval_samples_per_second': 15.774, 'eval_steps_per_second': 0.99, 'epoch': 3.0}\n",
      "{'train_runtime': 11611.2814, 'train_samples_per_second': 3.628, 'train_steps_per_second': 0.227, 'train_loss': 0.07772379602187496, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.07772379602187496, metrics={'train_runtime': 11611.2814, 'train_samples_per_second': 3.628, 'train_steps_per_second': 0.227, 'total_flos': 1020143109346326.0, 'train_loss': 0.07772379602187496, 'epoch': 3.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i,label in enumerate(label_list)}\n",
    "label2id = {label: str(i) for i,label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing using fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n",
    "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.99691856, 'index': 1, 'word': 'bill', 'start': 0, 'end': 4}, {'entity': 'I-PER', 'score': 0.99597245, 'index': 2, 'word': 'gates', 'start': 5, 'end': 10}, {'entity': 'B-ORG', 'score': 0.94493335, 'index': 7, 'word': 'microsoft', 'start': 29, 'end': 38}]\n"
     ]
    }
   ],
   "source": [
    "example = \"Bill Gates is the Founder of Microsoft\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
